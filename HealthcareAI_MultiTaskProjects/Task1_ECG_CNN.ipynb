{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Arrhythmia Classification from ECG using 1D CNN\n",
        "\n",
        "This notebook builds a complete ECG arrhythmia classifier using a 1D CNN. It downloads the Heartbeat dataset from Google Drive, preprocesses signals, trains a model with GPU, and evaluates with Accuracy, Precision, Recall, F1-score, and a Confusion Matrix.\n",
        "\n",
        "- Dataset: Google Drive (gdown): `https://drive.google.com/file/d/1xAs-CjlpuDqUT2EJUVR5cPuqTUdw2uQg/view?usp=sharing`\n",
        "- Model: 1D CNN\n",
        "- Framework: PyTorch\n",
        "- Runtime: GPU (recommended)\n",
        ""
      ],
      "id": "3daee0f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies (Colab-safe)\n",
        "import sys, subprocess, pkgutil\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    for p in pkgs:\n",
        "        try:\n",
        "            __import__(p.split('==')[0].split('>=')[0])\n",
        "        except Exception:\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', p])\n",
        "\n",
        "pip_install([\n",
        "    'gdown>=5.2.0', 'numpy>=1.24', 'pandas>=2.0', 'scipy>=1.11', 'matplotlib>=3.7', 'seaborn>=0.13',\n",
        "    'scikit-learn>=1.3', 'torch>=2.2', 'wfdb>=4.1.2'\n",
        "])\n",
        "\n",
        "import os, zipfile, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "print('Torch version:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8af7bd1f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download and extract dataset via gdown\n",
        "import gdown, os, zipfile\n",
        "\n",
        "GDRIVE_URL = 'https://drive.google.com/uc?id=1xAs-CjlpuDqUT2EJUVR5cPuqTUdw2uQg'\n",
        "DATA_ROOT = 'ecg_data'\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "zip_path = os.path.join(DATA_ROOT, 'heartbeat.zip')\n",
        "\n",
        "if not os.path.exists(zip_path):\n",
        "    gdown.download(GDRIVE_URL, zip_path, quiet=False)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "    zf.extractall(DATA_ROOT)\n",
        "\n",
        "print('Extracted to:', os.listdir(DATA_ROOT))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "bdf6ffe5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data understanding and preprocessing\n",
        "\n",
        "We will load the ECG beat segments into fixed-length windows, normalize per-sample, and encode class labels. If the dataset provides pre-segmented beats in `.csv` or `.npy`, we will parse them; otherwise, we will segment from raw signal files.\n",
        ""
      ],
      "id": "f06eba1e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Utility: load signals depending on distributed format\n",
        "# This block tries common formats: a) CSVs with beats and label column; b) npy arrays; c) folders per class\n",
        "import glob\n",
        "\n",
        "class_map = {}\n",
        "X, y = [], []\n",
        "\n",
        "# Try CSV files with columns: values..., label\n",
        "csv_files = glob.glob(os.path.join(DATA_ROOT, '**', '*.csv'), recursive=True)\n",
        "if csv_files:\n",
        "    import csv\n",
        "    for f in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(f)\n",
        "            # Heuristic: last column is label, others are signal values\n",
        "            if df.shape[1] > 1:\n",
        "                sig = df.iloc[:, :-1].values.astype('float32')\n",
        "                labels = df.iloc[:, -1].astype(str).values\n",
        "                for i in range(sig.shape[0]):\n",
        "                    X.append(sig[i])\n",
        "                    y.append(labels[i])\n",
        "        except Exception as e:\n",
        "            print('Skip CSV', f, e)\n",
        "\n",
        "# Try NPY files: expect dict with 'X','y' or two files\n",
        "if len(X) == 0:\n",
        "    npy_files = glob.glob(os.path.join(DATA_ROOT, '**', '*.npy'), recursive=True)\n",
        "    for f in npy_files:\n",
        "        try:\n",
        "            arr = np.load(f, allow_pickle=True)\n",
        "            if isinstance(arr, dict) and 'X' in arr and 'y' in arr:\n",
        "                X = [x.astype('float32') for x in arr['X']]\n",
        "                y = [str(v) for v in arr['y']]\n",
        "                break\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# Try folder per class with multiple .npy samples per file\n",
        "if len(X) == 0:\n",
        "    subdirs = [d for d in glob.glob(os.path.join(DATA_ROOT, '*')) if os.path.isdir(d)]\n",
        "    for d in subdirs:\n",
        "        class_name = os.path.basename(d)\n",
        "        npys = glob.glob(os.path.join(d, '*.npy'))\n",
        "        for f in npys:\n",
        "            try:\n",
        "                arr = np.load(f)\n",
        "                if arr.ndim == 1:\n",
        "                    X.append(arr.astype('float32'))\n",
        "                    y.append(class_name)\n",
        "                elif arr.ndim == 2:\n",
        "                    for row in arr:\n",
        "                        X.append(row.astype('float32'))\n",
        "                        y.append(class_name)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "print('Loaded samples:', len(X))\n",
        "assert len(X) == len(y) and len(X) > 0, 'No samples found; verify dataset structure after extraction.'\n",
        "\n",
        "# Pad/trim to fixed length\n",
        "TARGET_LEN = 187  # common for heartbeat segmentation datasets\n",
        "\n",
        "def pad_or_trim(signal, target_len=TARGET_LEN):\n",
        "    sig = np.asarray(signal, dtype='float32')\n",
        "    if sig.ndim > 1:\n",
        "        sig = sig.reshape(-1)\n",
        "    if len(sig) >= target_len:\n",
        "        return sig[:target_len]\n",
        "    out = np.zeros(target_len, dtype='float32')\n",
        "    out[:len(sig)] = sig\n",
        "    return out\n",
        "\n",
        "X = np.stack([pad_or_trim(x) for x in X])\n",
        "\n",
        "# Normalize per-sample to zero mean, unit variance (safe guard small std)\n",
        "means = X.mean(axis=1, keepdims=True)\n",
        "stds = X.std(axis=1, keepdims=True) + 1e-6\n",
        "X = (X - means) / stds\n",
        "\n",
        "# Encode labels\n",
        "unique_labels = sorted(list(set(y)))\n",
        "label_to_id = {lab: i for i, lab in enumerate(unique_labels)}\n",
        "id_to_label = {i: lab for lab, i in label_to_id.items()}\n",
        "y_ids = np.array([label_to_id[v] for v in y], dtype='int64')\n",
        "\n",
        "print('Classes:', unique_labels)\n",
        "print('X shape:', X.shape, 'y shape:', y_ids.shape)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b483f2cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train/val split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_ids, test_size=0.2, random_state=42, stratify=y_ids)\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        # shape (C, L) for 1D CNN; use C=1\n",
        "        return self.X[idx].unsqueeze(0), self.y[idx]\n",
        "\n",
        "train_ds = ECGDataset(X_train, y_train)\n",
        "val_ds = ECGDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "len(train_ds), len(val_ds)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8b72f7d6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define 1D CNN model\n",
        "class ECGCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, 1, TARGET_LEN)\n",
        "            out_len = self.features(dummy).shape[-1]\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "num_classes = len(unique_labels)\n",
        "model = ECGCNN(num_classes).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e3d4f163"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training loop with validation metrics\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    all_preds, all_trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            logits = model(xb)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_trues.append(yb.cpu().numpy())\n",
        "    y_true = np.concatenate(all_trues)\n",
        "    y_pred = np.concatenate(all_preds)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    return acc, pr, rc, f1, y_true, y_pred\n",
        "\n",
        "EPOCHS = 20\n",
        "best_f1 = 0.0\n",
        "history = []\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "    avg_loss = running_loss / len(train_ds)\n",
        "    acc, pr, rc, f1, y_true, y_pred = evaluate(model, val_loader)\n",
        "    scheduler.step(f1)\n",
        "    history.append({'epoch': epoch, 'loss': avg_loss, 'val_acc': acc, 'val_f1': f1})\n",
        "    print(f\"Epoch {epoch:02d} | loss {avg_loss:.4f} | val_acc {acc:.4f} | val_f1 {f1:.4f}\")\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        torch.save({'model_state': model.state_dict(), 'labels': id_to_label}, 'best_ecg_cnn.pt')\n",
        "\n",
        "print('Best F1:', best_f1)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "91ebbf35"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluation: confusion matrix and classification report\n",
        "acc, pr, rc, f1, y_true, y_pred = evaluate(model, val_loader)\n",
        "print('Final Validation Metrics:')\n",
        "print('Accuracy:', acc)\n",
        "print('Precision:', pr)\n",
        "print('Recall:', rc)\n",
        "print('F1-score:', f1)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_true, y_pred, target_names=unique_labels, zero_division=0))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4c78c1bb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notes on modeling choices\n",
        "\n",
        "- 1D CNN captures local morphological features of heartbeats efficiently.\n",
        "- Per-sample normalization stabilizes training across varying amplitudes.\n",
        "- Weighted macro metrics are reported to account for class imbalance. Consider class-weighted loss or focal loss if strongly imbalanced.\n",
        "- You can experiment with CNN+LSTM by adding a recurrent block after convolution features for temporal context.\n",
        ""
      ],
      "id": "1507275f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}