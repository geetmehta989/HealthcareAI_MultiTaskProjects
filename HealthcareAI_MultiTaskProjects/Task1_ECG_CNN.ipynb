{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Arrhythmia Classification Using 1D CNN\n",
        "\n",
        "This notebook builds an end-to-end ECG arrhythmia classifier using a 1D Convolutional Neural Network (PyTorch). It downloads the Heartbeat Dataset from Google Drive, preprocesses ECG beats, trains and validates a CNN, and reports Accuracy, Precision, Recall, F1-score, and a Confusion Matrix with plots. Run on a GPU runtime in Colab for best performance."
      ],
      "id": "db1026b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup: installs for Colab (safe to rerun)\n",
        "import sys, subprocess, pkgutil\n",
        "\n",
        "required = [\n",
        "    'gdown','numpy','pandas','scikit-learn','matplotlib','seaborn','wfdb','neurokit2',\n",
        "    'torch','torchvision','torchaudio'\n",
        "]\n",
        "\n",
        "for pkg in required:\n",
        "    if pkg not in {m.name for m in pkgutil.iter_modules()}:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DEVICE"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "23d9a4cb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download dataset from Google Drive\n",
        "\n",
        "- Public link provided. If access is restricted, authenticate in Colab.\n",
        "- We download the zip via `gdown` and extract.\n",
        "- If Colab Drive mount is preferred, you can skip `gdown` and copy the file manually."
      ],
      "id": "353dcc0e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, zipfile, pathlib, sys\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path('data_ecg')\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "ZIP_PATH = DATA_DIR / 'ecg_heartbeat.zip'\n",
        "EXTRACT_DIR = DATA_DIR / 'extracted'\n",
        "EXTRACT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Google Drive file id from the provided link\n",
        "GDRIVE_ID = '1xAs-CjlpuDqUT2EJUVR5cPuqTUdw2uQg'\n",
        "\n",
        "try:\n",
        "    import gdown\n",
        "    if not ZIP_PATH.exists():\n",
        "        url = f'https://drive.google.com/uc?id={GDRIVE_ID}'\n",
        "        gdown.download(url, str(ZIP_PATH), quiet=False)\n",
        "    # Extract\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "        zf.extractall(EXTRACT_DIR)\n",
        "except Exception as e:\n",
        "    print('Download or extraction failed:', e)\n",
        "    print('If in Colab, try authenticating Google Drive and copy the file to', ZIP_PATH)\n",
        "\n",
        "list(EXTRACT_DIR.glob('**/*'))[:10]"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7e598969"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing\n",
        "\n",
        "- Load ECG beats and labels from extracted directory.\n",
        "- Normalize each beat to zero mean, unit variance.\n",
        "- Pad/trim to a fixed length if needed.\n",
        "- Create train/validation/test splits."
      ],
      "id": "fd0ba665"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Heuristic loader for common heartbeat dataset formats (e.g., Kaggle heartbeat, MIT-BIH beat slices)\n",
        "# Expecting structure like: EXTRACT_DIR/<class_name>/*.csv or a master CSV with label column.\n",
        "\n",
        "def discover_files(root: Path):\n",
        "    csvs = list(root.glob('**/*.csv'))\n",
        "    npys = list(root.glob('**/*.npy'))\n",
        "    return csvs, npys\n",
        "\n",
        "csvs, npys = discover_files(EXTRACT_DIR)\n",
        "print('Found CSVs:', len(csvs), 'NPYs:', len(npys))\n",
        "\n",
        "# Try to load into (samples, length) and labels\n",
        "X_list, y_list = [], []\n",
        "label_to_id = {}\n",
        "\n",
        "if csvs:\n",
        "    # Case 1: directory per class or master CSV with label column\n",
        "    for csv_path in csvs:\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "        except Exception:\n",
        "            continue\n",
        "        # If a label column exists\n",
        "        label_col = None\n",
        "        for cand in ['label','class','Class','y','target']:\n",
        "            if cand in df.columns:\n",
        "                label_col = cand\n",
        "                break\n",
        "        if label_col is not None and df.shape[0] > 0:\n",
        "            # Treat rows as samples\n",
        "            labels = df[label_col].values\n",
        "            features = df.drop(columns=[label_col]).values\n",
        "            # Clean NaNs\n",
        "            features = np.nan_to_num(features)\n",
        "            X_list.append(features)\n",
        "            y_list.append(labels)\n",
        "        else:\n",
        "            # Single-sample CSVs in class directories\n",
        "            # Infer label from parent directory name\n",
        "            label_name = csv_path.parent.name\n",
        "            arr = pd.read_csv(csv_path, header=None).values.flatten()\n",
        "            X_list.append(arr[None, :])\n",
        "            y_list.append(np.array([label_name]))\n",
        "elif npys:\n",
        "    # If npy files exist; infer label from filename or parent\n",
        "    for npy_path in npys:\n",
        "        arr = np.load(npy_path)\n",
        "        if arr.ndim == 1:\n",
        "            arr = arr[None, :]\n",
        "        label_name = npy_path.parent.name\n",
        "        X_list.append(arr)\n",
        "        y_list.append(np.array([label_name]*arr.shape[0]))\n",
        "else:\n",
        "    raise RuntimeError('No CSV or NPY files found. Please verify dataset structure after extraction.')\n",
        "\n",
        "X = np.vstack(X_list)\n",
        "y = np.concatenate(y_list)\n",
        "\n",
        "# Encode labels to integers\n",
        "classes = sorted(np.unique(y))\n",
        "class_to_id = {c:i for i,c in enumerate(classes)}\n",
        "y_ids = np.array([class_to_id[c] for c in y], dtype=np.int64)\n",
        "\n",
        "# Standardize each sample\n",
        "X = np.nan_to_num(X)\n",
        "X = (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, keepdims=True) + 1e-6)\n",
        "\n",
        "# Ensure fixed length (pad/trim) to max_len\n",
        "max_len = int(np.percentile([len(x) for x in X], 95)) if X.ndim == 2 else X.shape[1]\n",
        "\n",
        "def pad_trim(arr, L):\n",
        "    if arr.shape[-1] == L:\n",
        "        return arr\n",
        "    if arr.shape[-1] > L:\n",
        "        return arr[..., :L]\n",
        "    out = np.zeros((arr.shape[0], L), dtype=arr.dtype)\n",
        "    out[:, :arr.shape[1]] = arr\n",
        "    return out\n",
        "\n",
        "if X.ndim == 1:\n",
        "    X = X[None, :]\n",
        "X = pad_trim(X, max_len)\n",
        "\n",
        "print('Data shape:', X.shape, 'Num classes:', len(classes))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "82456671"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train/Val/Test split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y_ids, test_size=0.3, random_state=42, stratify=y_ids)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "len(X_train), len(X_val), len(X_test)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ca0929f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        if self.X.ndim == 2:\n",
        "            self.X = self.X.unsqueeze(1)  # (N, 1, L)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_ds = ECGDataset(X_train, y_train)\n",
        "val_ds = ECGDataset(X_val, y_val)\n",
        "test_ds = ECGDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n",
        "len(train_ds), len(val_ds), len(test_ds)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "29d7b425"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ECG1DCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.feature(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = ECG1DCNN(num_classes=len(classes)).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5f9053c6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += xb.size(0)\n",
        "    return running_loss/total, correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += xb.size(0)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(yb.cpu().numpy())\n",
        "    avg_loss = running_loss/total\n",
        "    acc = correct/total\n",
        "    return avg_loss, acc, np.concatenate(all_preds), np.concatenate(all_labels)\n",
        "\n",
        "EPOCHS = 20\n",
        "best_val_acc = 0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
        "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, DEVICE)\n",
        "    print(f'Epoch {epoch:02d}: train_loss={tr_loss:.4f} train_acc={tr_acc:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f}')\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict({k:v.to(DEVICE) for k,v in best_state.items()})\n",
        ""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d2491d6c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final evaluation on test set\n",
        "te_loss, te_acc, te_preds, te_labels = evaluate(model, test_loader, criterion, DEVICE)\n",
        "print('Test loss:', te_loss, 'Test acc:', te_acc)\n",
        "\n",
        "print(classification_report(te_labels, te_preds, target_names=classes, digits=4))\n",
        "cm = confusion_matrix(te_labels, te_preds)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "01d48cbd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes and References\n",
        "\n",
        "- We used a compact 1D CNN suited for ECG beats. Optionally, add a bidirectional LSTM after the convolutional stack for sequential context.\n",
        "- Handle class imbalance via class weights or focal loss if needed.\n",
        "- Reference tutorial inspiration: CNN for arrhythmia detection (various PyTorch/TF guides on Medium/Kaggle).\n",
        ""
      ],
      "id": "6e9482ac"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}