{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: LLaMA 3.1 Text Summarization\n",
    "\n",
    "## Objective\n",
    "Fine-tune LLaMA 3.1 (or substitute model) for abstractive text summarization using the CNN/DailyMail dataset.\n",
    "\n",
    "## Dataset\n",
    "- **Source**: CNN/DailyMail Summarization Dataset from Kaggle\n",
    "- **URL**: https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail\n",
    "- **Task**: Abstractive text summarization\n",
    "\n",
    "## Approach\n",
    "1. Download CNN/DailyMail dataset using Kaggle API\n",
    "2. Preprocess articles and summaries for sequence-to-sequence learning\n",
    "3. Fine-tune LLaMA 3.1 (or substitute like LLaMA 2 or Mistral) using LoRA/QLoRA\n",
    "4. Evaluate using ROUGE and BLEU metrics\n",
    "5. Generate example summaries and analyze model performance\n",
    "\n",
    "**Note**: Due to LLaMA 3.1's size and potential access restrictions, we'll use a more accessible model like `microsoft/DialoGPT-medium` or `facebook/bart-large-cnn` as a substitute while maintaining the same methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets accelerate torch torchvision pandas numpy scikit-learn matplotlib seaborn plotly tqdm evaluate rouge-score nltk kaggle peft bitsandbytes\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Transformers and datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Evaluation metrics\n",
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "# Check available memory for model selection\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"\\nGPU Memory: {gpu_memory_gb:.1f} GB\")\n",
    "    if gpu_memory_gb >= 24:\n",
    "        print(\"Sufficient memory for large models\")\n",
    "    elif gpu_memory_gb >= 12:\n",
    "        print(\"Medium memory - will use quantization\")\n",
    "    else:\n",
    "        print(\"Limited memory - will use smaller model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Download and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Download from Kaggle (requires API setup)\n",
    "# Uncomment and configure if you have Kaggle API credentials\n",
    "\n",
    "# import kaggle\n",
    "# kaggle.api.authenticate()\n",
    "# kaggle.api.dataset_download_files('gowrishankarp/newspaper-text-summarization-cnn-dailymail', \n",
    "#                                   path='./', unzip=True)\n",
    "\n",
    "# Option 2: Use Hugging Face datasets (more reliable for Colab)\n",
    "print(\"Loading CNN/DailyMail dataset from Hugging Face...\")\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    # Try to load the full dataset\n",
    "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "    print(\"Successfully loaded full CNN/DailyMail dataset\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading full dataset: {e}\")\n",
    "    print(\"Loading a smaller subset for demonstration...\")\n",
    "    \n",
    "    # Create a smaller synthetic dataset for demonstration\n",
    "    sample_data = {\n",
    "        'article': [\n",
    "            \"The World Health Organization (WHO) announced today that a new variant of the coronavirus has been detected in several countries. The variant, named Omicron, has multiple mutations in the spike protein that could potentially affect transmissibility and vaccine effectiveness. Scientists are currently studying the variant to understand its characteristics better. Initial reports suggest that the variant may be more transmissible than previous variants, but more research is needed to confirm this. Health officials are urging continued vigilance and adherence to public health measures including vaccination, mask-wearing, and social distancing. The WHO has classified this variant as a variant of concern due to its potential impact on public health.\",\n",
    "            \n",
    "            \"Climate change continues to be one of the most pressing issues of our time. Recent studies show that global temperatures have risen by 1.1 degrees Celsius since pre-industrial times. The effects are already being felt worldwide, with more frequent extreme weather events, rising sea levels, and changing precipitation patterns. Scientists warn that without immediate action to reduce greenhouse gas emissions, the consequences could be catastrophic. Renewable energy sources like solar and wind power are becoming increasingly cost-effective alternatives to fossil fuels. Many countries have committed to achieving net-zero emissions by 2050, but experts say more ambitious targets and faster implementation are needed.\",\n",
    "            \n",
    "            \"Artificial intelligence technology is rapidly advancing across multiple sectors. Recent breakthroughs in machine learning have enabled computers to perform tasks that were previously thought to require human intelligence. From medical diagnosis to autonomous vehicles, AI is transforming how we work and live. However, experts also warn about potential risks including job displacement, privacy concerns, and the need for ethical AI development. Tech companies are investing billions of dollars in AI research and development. Governments worldwide are working to establish regulations and guidelines for AI use to ensure it benefits society while minimizing potential harms.\",\n",
    "            \n",
    "            \"The global economy is showing signs of recovery following the pandemic-induced recession. GDP growth has returned to positive territory in most developed countries, though challenges remain. Supply chain disruptions continue to affect various industries, leading to shortages and increased prices for consumer goods. Central banks are carefully monitoring inflation rates and adjusting monetary policies accordingly. Employment levels are gradually improving, but some sectors are still struggling to find workers. Economists predict that full economic recovery may take several more years, with ongoing uncertainty about future pandemic impacts and geopolitical tensions.\",\n",
    "            \n",
    "            \"Space exploration reached new milestones this year with successful missions to Mars and the launch of the James Webb Space Telescope. NASA's Perseverance rover has been collecting samples on Mars that may contain evidence of ancient microbial life. The Webb telescope has already captured stunning images of distant galaxies, providing new insights into the early universe. Private space companies are also making significant progress, with SpaceX conducting regular missions to the International Space Station. Plans for future lunar missions and eventual human missions to Mars are becoming more concrete, marking a new era in space exploration.\"\n",
    "        ],\n",
    "        'highlights': [\n",
    "            \"WHO announces detection of new coronavirus variant Omicron with multiple spike protein mutations. Variant classified as concern due to potential increased transmissibility. Health officials urge continued public health measures.\",\n",
    "            \n",
    "            \"Global temperatures have risen 1.1Â°C since pre-industrial times due to climate change. Extreme weather events increasing worldwide. Scientists call for immediate action to reduce emissions and achieve net-zero by 2050.\",\n",
    "            \n",
    "            \"AI technology advancing rapidly across sectors with breakthroughs in machine learning. Applications include medical diagnosis and autonomous vehicles. Experts warn of risks including job displacement and privacy concerns.\",\n",
    "            \n",
    "            \"Global economy showing recovery signs with positive GDP growth in developed countries. Supply chain disruptions causing shortages and price increases. Full recovery expected to take several more years.\",\n",
    "            \n",
    "            \"Space exploration achieves milestones with Mars missions and Webb telescope launch. Perseverance rover collecting Mars samples. Private companies advancing space technology for future lunar and Mars missions.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Expand the dataset by creating variations\n",
    "    expanded_articles = []\n",
    "    expanded_highlights = []\n",
    "    \n",
    "    for i in range(200):  # Create 200 samples\n",
    "        idx = i % len(sample_data['article'])\n",
    "        expanded_articles.append(sample_data['article'][idx])\n",
    "        expanded_highlights.append(sample_data['highlights'][idx])\n",
    "    \n",
    "    # Create dataset splits\n",
    "    train_size = int(0.8 * len(expanded_articles))\n",
    "    val_size = int(0.1 * len(expanded_articles))\n",
    "    \n",
    "    dataset = DatasetDict({\n",
    "        'train': Dataset.from_dict({\n",
    "            'article': expanded_articles[:train_size],\n",
    "            'highlights': expanded_highlights[:train_size]\n",
    "        }),\n",
    "        'validation': Dataset.from_dict({\n",
    "            'article': expanded_articles[train_size:train_size+val_size],\n",
    "            'highlights': expanded_highlights[train_size:train_size+val_size]\n",
    "        }),\n",
    "        'test': Dataset.from_dict({\n",
    "            'article': expanded_articles[train_size+val_size:],\n",
    "            'highlights': expanded_highlights[train_size+val_size:]\n",
    "        })\n",
    "    })\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n",
    "\n",
    "# Display sample\n",
    "sample = dataset['train'][0]\n",
    "print(f\"\\nSample article (first 200 chars): {sample['article'][:200]}...\")\n",
    "print(f\"Sample summary: {sample['highlights']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths\n",
    "def analyze_lengths(dataset_split, split_name):\n",
    "    articles = dataset_split['article']\n",
    "    summaries = dataset_split['highlights']\n",
    "    \n",
    "    article_lengths = [len(article.split()) for article in articles]\n",
    "    summary_lengths = [len(summary.split()) for summary in summaries]\n",
    "    \n",
    "    print(f\"\\n{split_name} Statistics:\")\n",
    "    print(f\"Article lengths - Mean: {np.mean(article_lengths):.1f}, Max: {np.max(article_lengths)}, Min: {np.min(article_lengths)}\")\n",
    "    print(f\"Summary lengths - Mean: {np.mean(summary_lengths):.1f}, Max: {np.max(summary_lengths)}, Min: {np.min(summary_lengths)}\")\n",
    "    \n",
    "    return article_lengths, summary_lengths\n",
    "\n",
    "# Analyze all splits\n",
    "train_art_lens, train_sum_lens = analyze_lengths(dataset['train'], 'Train')\n",
    "val_art_lens, val_sum_lens = analyze_lengths(dataset['validation'], 'Validation')\n",
    "test_art_lens, test_sum_lens = analyze_lengths(dataset['test'], 'Test')\n",
    "\n",
    "# Visualize length distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Article lengths\n",
    "axes[0, 0].hist(train_art_lens, bins=30, alpha=0.7, label='Train')\n",
    "axes[0, 0].hist(val_art_lens, bins=30, alpha=0.7, label='Validation')\n",
    "axes[0, 0].hist(test_art_lens, bins=30, alpha=0.7, label='Test')\n",
    "axes[0, 0].set_title('Article Length Distribution (Words)')\n",
    "axes[0, 0].set_xlabel('Number of Words')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Summary lengths\n",
    "axes[0, 1].hist(train_sum_lens, bins=30, alpha=0.7, label='Train')\n",
    "axes[0, 1].hist(val_sum_lens, bins=30, alpha=0.7, label='Validation')\n",
    "axes[0, 1].hist(test_sum_lens, bins=30, alpha=0.7, label='Test')\n",
    "axes[0, 1].set_title('Summary Length Distribution (Words)')\n",
    "axes[0, 1].set_xlabel('Number of Words')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Compression ratio\n",
    "compression_ratios = [s_len / a_len for a_len, s_len in zip(train_art_lens, train_sum_lens)]\n",
    "axes[1, 0].hist(compression_ratios, bins=30, alpha=0.7)\n",
    "axes[1, 0].set_title('Compression Ratio Distribution')\n",
    "axes[1, 0].set_xlabel('Summary Length / Article Length')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Length relationship scatter plot\n",
    "axes[1, 1].scatter(train_art_lens[:50], train_sum_lens[:50], alpha=0.6)\n",
    "axes[1, 1].set_title('Article vs Summary Length Relationship')\n",
    "axes[1, 1].set_xlabel('Article Length (Words)')\n",
    "axes[1, 1].set_ylabel('Summary Length (Words)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCompression Statistics:\")\n",
    "print(f\"Mean compression ratio: {np.mean(compression_ratios):.3f}\")\n",
    "print(f\"Median compression ratio: {np.median(compression_ratios):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Selection and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection based on available resources\n",
    "# We'll use BART as it's specifically designed for summarization and more accessible than LLaMA\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"  # Pre-trained on CNN/DailyMail\n",
    "# Alternative models:\n",
    "# \"google/pegasus-cnn_dailymail\" - Another good summarization model\n",
    "# \"microsoft/DialoGPT-medium\" - For causal LM approach\n",
    "# \"t5-base\" - For T5-based summarization\n",
    "\n",
    "print(f\"Selected model: {model_name}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded successfully\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Max length: {tokenizer.model_max_length}\")\n",
    "\n",
    "# Determine if we need quantization\n",
    "use_quantization = False\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    if gpu_memory_gb < 16:  # Use quantization for limited memory\n",
    "        use_quantization = True\n",
    "        print(\"Using 4-bit quantization due to memory constraints\")\n",
    "\n",
    "# Load model with optional quantization\n",
    "if use_quantization:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded successfully\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set maximum lengths based on model and dataset characteristics\n",
    "max_input_length = 1024  # Maximum article length\n",
    "max_target_length = 128  # Maximum summary length\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the data for sequence-to-sequence training.\"\"\"\n",
    "    \n",
    "    # Get the articles and summaries\n",
    "    articles = examples['article']\n",
    "    summaries = examples['highlights']\n",
    "    \n",
    "    # Tokenize inputs (articles)\n",
    "    model_inputs = tokenizer(\n",
    "        articles,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets (summaries)\n",
    "    labels = tokenizer(\n",
    "        summaries,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Add labels to model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to all dataset splits\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "print(\"Tokenization completed!\")\n",
    "print(f\"Tokenized train dataset: {tokenized_datasets['train']}\")\n",
    "\n",
    "# Analyze tokenized lengths\n",
    "train_input_lengths = [len(item['input_ids']) for item in tokenized_datasets['train']]\n",
    "train_label_lengths = [len(item['labels']) for item in tokenized_datasets['train']]\n",
    "\n",
    "print(f\"\\nTokenized length statistics:\")\n",
    "print(f\"Input lengths - Mean: {np.mean(train_input_lengths):.1f}, Max: {np.max(train_input_lengths)}\")\n",
    "print(f\"Label lengths - Mean: {np.mean(train_label_lengths):.1f}, Max: {np.max(train_label_lengths)}\")\n",
    "\n",
    "# Visualize tokenized lengths\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_input_lengths, bins=30, alpha=0.7)\n",
    "plt.title('Tokenized Input Length Distribution')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(max_input_length, color='red', linestyle='--', label=f'Max Length ({max_input_length})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(train_label_lengths, bins=30, alpha=0.7)\n",
    "plt.title('Tokenized Target Length Distribution')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(max_target_length, color='red', linestyle='--', label=f'Max Length ({max_target_length})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LoRA Configuration (for Large Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for parameter-efficient fine-tuning\n",
    "# This is especially useful for large models like LLaMA\n",
    "\n",
    "use_lora = True  # Set to True to use LoRA, False for full fine-tuning\n",
    "\n",
    "if use_lora:\n",
    "    print(\"Configuring LoRA for parameter-efficient fine-tuning...\")\n",
    "    \n",
    "    # LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,  # For sequence-to-sequence models\n",
    "        r=16,  # Rank of adaptation\n",
    "        lora_alpha=32,  # LoRA scaling parameter\n",
    "        lora_dropout=0.1,  # LoRA dropout\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\", \n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"fc1\",\n",
    "            \"fc2\"\n",
    "        ]  # Target modules for LoRA\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to the model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    print(\"LoRA configuration applied successfully!\")\n",
    "else:\n",
    "    print(\"Using full fine-tuning (no LoRA)\")\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute ROUGE and BLEU metrics for evaluation.\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in labels (used for padding) with pad token\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up text\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    rouge_result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    # Convert to list of lists for BLEU (multiple references per prediction)\n",
    "    bleu_references = [[label] for label in decoded_labels]\n",
    "    bleu_result = bleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=bleu_references\n",
    "    )\n",
    "    \n",
    "    # Combine results\n",
    "    result = {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"bleu\": bleu_result[\"bleu\"]\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./summarization_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,  # Smaller batch size for memory efficiency\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,  # Effective batch size = 4 * 2 = 8\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=None,\n",
    "    seed=42,\n",
    "    fp16=torch.cuda.is_available() and not use_quantization,\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False,\n",
    "    predict_with_generate=True,  # Important for seq2seq models\n",
    "    generation_max_length=max_target_length,\n",
    "    generation_num_beams=4  # Use beam search for better generation\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Batch size (per device): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Mixed precision (fp16): {training_args.fp16}\")\n",
    "print(f\"Generation max length: {training_args.generation_max_length}\")\n",
    "print(f\"Generation num beams: {training_args.generation_num_beams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Total training steps: {len(tokenized_datasets['train']) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training steps: {train_result.global_step}\")\n",
    "\n",
    "# Save the model\n",
    "if use_lora:\n",
    "    # Save LoRA adapter\n",
    "    model.save_pretrained(\"./best_summarization_lora\")\n",
    "    tokenizer.save_pretrained(\"./best_summarization_lora\")\n",
    "    print(\"LoRA adapter saved to './best_summarization_lora'\")\n",
    "else:\n",
    "    # Save full model\n",
    "    trainer.save_model(\"./best_summarization_model\")\n",
    "    tokenizer.save_pretrained(\"./best_summarization_model\")\n",
    "    print(\"Model saved to './best_summarization_model'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Separate training and evaluation logs\n",
    "train_logs = [log for log in log_history if 'loss' in log and 'eval_loss' not in log]\n",
    "eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "if train_logs and eval_logs:\n",
    "    # Extract metrics\n",
    "    train_steps = [log['step'] for log in train_logs]\n",
    "    train_losses = [log['loss'] for log in train_logs]\n",
    "    \n",
    "    eval_steps = [log['step'] for log in eval_logs]\n",
    "    eval_losses = [log['eval_loss'] for log in eval_logs]\n",
    "    eval_rouge1 = [log.get('eval_rouge1', 0) for log in eval_logs]\n",
    "    eval_rouge2 = [log.get('eval_rouge2', 0) for log in eval_logs]\n",
    "    eval_rougeL = [log.get('eval_rougeL', 0) for log in eval_logs]\n",
    "    eval_bleu = [log.get('eval_bleu', 0) for log in eval_logs]\n",
    "    \n",
    "    # Plot training history\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Training and validation loss\n",
    "    axes[0, 0].plot(train_steps, train_losses, 'b-', label='Training Loss')\n",
    "    axes[0, 0].plot(eval_steps, eval_losses, 'r-', label='Validation Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Steps')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ROUGE-1\n",
    "    axes[0, 1].plot(eval_steps, eval_rouge1, 'g-', label='ROUGE-1')\n",
    "    axes[0, 1].set_title('ROUGE-1 Score')\n",
    "    axes[0, 1].set_xlabel('Steps')\n",
    "    axes[0, 1].set_ylabel('ROUGE-1')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ROUGE-2\n",
    "    axes[0, 2].plot(eval_steps, eval_rouge2, 'orange', label='ROUGE-2')\n",
    "    axes[0, 2].set_title('ROUGE-2 Score')\n",
    "    axes[0, 2].set_xlabel('Steps')\n",
    "    axes[0, 2].set_ylabel('ROUGE-2')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ROUGE-L\n",
    "    axes[1, 0].plot(eval_steps, eval_rougeL, 'purple', label='ROUGE-L')\n",
    "    axes[1, 0].set_title('ROUGE-L Score')\n",
    "    axes[1, 0].set_xlabel('Steps')\n",
    "    axes[1, 0].set_ylabel('ROUGE-L')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # BLEU\n",
    "    axes[1, 1].plot(eval_steps, eval_bleu, 'brown', label='BLEU')\n",
    "    axes[1, 1].set_title('BLEU Score')\n",
    "    axes[1, 1].set_xlabel('Steps')\n",
    "    axes[1, 1].set_ylabel('BLEU')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # All ROUGE scores together\n",
    "    axes[1, 2].plot(eval_steps, eval_rouge1, 'g-', label='ROUGE-1')\n",
    "    axes[1, 2].plot(eval_steps, eval_rouge2, 'orange', label='ROUGE-2')\n",
    "    axes[1, 2].plot(eval_steps, eval_rougeL, 'purple', label='ROUGE-L')\n",
    "    axes[1, 2].set_title('All ROUGE Scores')\n",
    "    axes[1, 2].set_xlabel('Steps')\n",
    "    axes[1, 2].set_ylabel('Score')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Interactive plot with Plotly\n",
    "    fig_plotly = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Loss', 'ROUGE Scores', 'BLEU Score', 'Learning Rate'),\n",
    "    )\n",
    "    \n",
    "    # Add traces\n",
    "    fig_plotly.add_trace(go.Scatter(x=train_steps, y=train_losses, mode='lines', name='Train Loss'), row=1, col=1)\n",
    "    fig_plotly.add_trace(go.Scatter(x=eval_steps, y=eval_losses, mode='lines', name='Val Loss'), row=1, col=1)\n",
    "    \n",
    "    fig_plotly.add_trace(go.Scatter(x=eval_steps, y=eval_rouge1, mode='lines', name='ROUGE-1'), row=1, col=2)\n",
    "    fig_plotly.add_trace(go.Scatter(x=eval_steps, y=eval_rouge2, mode='lines', name='ROUGE-2'), row=1, col=2)\n",
    "    fig_plotly.add_trace(go.Scatter(x=eval_steps, y=eval_rougeL, mode='lines', name='ROUGE-L'), row=1, col=2)\n",
    "    \n",
    "    fig_plotly.add_trace(go.Scatter(x=eval_steps, y=eval_bleu, mode='lines', name='BLEU'), row=2, col=1)\n",
    "    \n",
    "    fig_plotly.update_layout(height=600, showlegend=True, title_text=\"Training History\")\n",
    "    fig_plotly.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Training history not available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "for key, value in test_results.items():\n",
    "    if key.startswith('eval_'):\n",
    "        metric_name = key.replace('eval_', '').upper()\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "# Get detailed predictions for analysis\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "decoded_preds = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)\n",
    "\n",
    "# Get original test data for comparison\n",
    "test_articles = dataset[\"test\"][\"article\"]\n",
    "test_summaries = dataset[\"test\"][\"highlights\"]\n",
    "\n",
    "print(f\"\\nGenerated {len(decoded_preds)} summaries for evaluation\")\n",
    "\n",
    "# Calculate additional metrics manually for verification\n",
    "rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for pred, ref in zip(decoded_preds, test_summaries):\n",
    "    scores = rouge_scorer_obj.score(ref, pred)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "print(f\"\\nDetailed ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {np.mean(rouge1_scores):.4f} (Â±{np.std(rouge1_scores):.4f})\")\n",
    "print(f\"ROUGE-2: {np.mean(rouge2_scores):.4f} (Â±{np.std(rouge2_scores):.4f})\")\n",
    "print(f\"ROUGE-L: {np.mean(rougeL_scores):.4f} (Â±{np.std(rougeL_scores):.4f})\")\n",
    "\n",
    "# Visualize score distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(rouge1_scores, bins=20, alpha=0.7, color='green')\n",
    "plt.title('ROUGE-1 Score Distribution')\n",
    "plt.xlabel('ROUGE-1 Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(np.mean(rouge1_scores), color='red', linestyle='--', label=f'Mean: {np.mean(rouge1_scores):.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(rouge2_scores, bins=20, alpha=0.7, color='orange')\n",
    "plt.title('ROUGE-2 Score Distribution')\n",
    "plt.xlabel('ROUGE-2 Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(np.mean(rouge2_scores), color='red', linestyle='--', label=f'Mean: {np.mean(rouge2_scores):.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(rougeL_scores, bins=20, alpha=0.7, color='purple')\n",
    "plt.title('ROUGE-L Score Distribution')\n",
    "plt.xlabel('ROUGE-L Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(np.mean(rougeL_scores), color='red', linestyle='--', label=f'Mean: {np.mean(rougeL_scores):.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Example Summaries and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example summaries\n",
    "def show_example_summaries(num_examples=5):\n",
    "    \"\"\"Display example summaries with quality scores.\"\"\"\n",
    "    \n",
    "    print(\"Example Summaries:\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    for i in range(min(num_examples, len(decoded_preds))):\n",
    "        article = test_articles[i]\n",
    "        reference = test_summaries[i]\n",
    "        generated = decoded_preds[i]\n",
    "        \n",
    "        # Calculate ROUGE scores for this example\n",
    "        scores = rouge_scorer_obj.score(reference, generated)\n",
    "        \n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Article (first 300 chars): {article[:300]}...\")\n",
    "        print(f\"\\nReference Summary: {reference}\")\n",
    "        print(f\"\\nGenerated Summary: {generated}\")\n",
    "        print(f\"\\nROUGE Scores:\")\n",
    "        print(f\"  ROUGE-1: {scores['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"  ROUGE-2: {scores['rouge2'].fmeasure:.4f}\")\n",
    "        print(f\"  ROUGE-L: {scores['rougeL'].fmeasure:.4f}\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "show_example_summaries(5)\n",
    "\n",
    "# Analyze summary characteristics\n",
    "pred_lengths = [len(pred.split()) for pred in decoded_preds]\n",
    "ref_lengths = [len(ref.split()) for ref in test_summaries]\n",
    "\n",
    "print(f\"\\nSummary Length Analysis:\")\n",
    "print(f\"Generated summaries - Mean length: {np.mean(pred_lengths):.1f} words\")\n",
    "print(f\"Reference summaries - Mean length: {np.mean(ref_lengths):.1f} words\")\n",
    "print(f\"Length difference: {np.mean(pred_lengths) - np.mean(ref_lengths):.1f} words\")\n",
    "\n",
    "# Visualize length comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(pred_lengths, bins=20, alpha=0.7, label='Generated', color='blue')\n",
    "plt.hist(ref_lengths, bins=20, alpha=0.7, label='Reference', color='red')\n",
    "plt.title('Summary Length Distribution')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(ref_lengths, pred_lengths, alpha=0.6)\n",
    "plt.plot([0, max(max(ref_lengths), max(pred_lengths))], \n",
    "         [0, max(max(ref_lengths), max(pred_lengths))], 'r--', label='Perfect Match')\n",
    "plt.title('Generated vs Reference Length')\n",
    "plt.xlabel('Reference Length (words)')\n",
    "plt.ylabel('Generated Length (words)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Interactive Summary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating summaries from new text\n",
    "def generate_summary(text, max_length=128, num_beams=4, length_penalty=2.0):\n",
    "    \"\"\"Generate summary for a given text.\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            length_penalty=length_penalty,\n",
    "            early_stopping=True,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    # Decode summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Test with custom examples\n",
    "custom_articles = [\n",
    "    \"\"\"Scientists have made a groundbreaking discovery in the field of renewable energy. \n",
    "    Researchers at MIT have developed a new type of solar cell that can achieve 47% efficiency, \n",
    "    significantly higher than current commercial solar panels which typically achieve 20-22% efficiency. \n",
    "    The breakthrough involves using a new material called perovskite in combination with traditional silicon cells. \n",
    "    This tandem approach allows the cell to capture a broader spectrum of sunlight. \n",
    "    The researchers believe this technology could revolutionize the solar energy industry and make \n",
    "    renewable energy more cost-effective than fossil fuels. However, challenges remain in scaling \n",
    "    up production and ensuring long-term stability of the perovskite material.\"\"\",\n",
    "    \n",
    "    \"\"\"The global food crisis is worsening as climate change and geopolitical conflicts disrupt \n",
    "    supply chains worldwide. According to the United Nations, over 800 million people are currently \n",
    "    facing acute food insecurity, with the situation expected to deteriorate further. \n",
    "    The war in Ukraine has significantly impacted grain exports, while droughts in East Africa \n",
    "    and floods in Pakistan have destroyed crops. Rising fertilizer costs due to energy price \n",
    "    increases are also affecting agricultural productivity globally. International organizations \n",
    "    are calling for immediate action to prevent famine in the most affected regions. \n",
    "    Solutions being proposed include emergency food aid, investment in climate-resilient agriculture, \n",
    "    and diplomatic efforts to ensure safe passage of food shipments.\"\"\"\n",
    "]\n",
    "\n",
    "print(\"Custom Summary Generation Examples:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, article in enumerate(custom_articles):\n",
    "    print(f\"\\nArticle {i+1}:\")\n",
    "    print(f\"Original text: {article[:200]}...\")\n",
    "    \n",
    "    summary = generate_summary(article)\n",
    "    print(f\"\\nGenerated Summary: {summary}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Compare different generation parameters\n",
    "test_article = custom_articles[0]\n",
    "\n",
    "print(f\"\\nParameter Comparison for Same Article:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Different beam sizes\n",
    "for num_beams in [2, 4, 8]:\n",
    "    summary = generate_summary(test_article, num_beams=num_beams)\n",
    "    print(f\"\\nBeams={num_beams}: {summary}\")\n",
    "\n",
    "# Different length penalties\n",
    "print(f\"\\nLength Penalty Comparison:\")\n",
    "for length_penalty in [1.0, 2.0, 3.0]:\n",
    "    summary = generate_summary(test_article, length_penalty=length_penalty)\n",
    "    print(f\"\\nLength Penalty={length_penalty}: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Analysis and Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance across different article lengths\n",
    "article_lengths = [len(article.split()) for article in test_articles]\n",
    "\n",
    "# Create length bins\n",
    "length_bins = [(0, 200), (200, 400), (400, 600), (600, float('inf'))]\n",
    "bin_labels = ['Short (0-200)', 'Medium (200-400)', 'Long (400-600)', 'Very Long (600+)']\n",
    "\n",
    "bin_rouge1_scores = []\n",
    "bin_rouge2_scores = []\n",
    "bin_rougeL_scores = []\n",
    "bin_counts = []\n",
    "\n",
    "for min_len, max_len in length_bins:\n",
    "    # Find articles in this length range\n",
    "    indices = [i for i, length in enumerate(article_lengths) \n",
    "               if min_len <= length < max_len]\n",
    "    \n",
    "    if indices:\n",
    "        bin_rouge1 = [rouge1_scores[i] for i in indices]\n",
    "        bin_rouge2 = [rouge2_scores[i] for i in indices]\n",
    "        bin_rougeL = [rougeL_scores[i] for i in indices]\n",
    "        \n",
    "        bin_rouge1_scores.append(np.mean(bin_rouge1))\n",
    "        bin_rouge2_scores.append(np.mean(bin_rouge2))\n",
    "        bin_rougeL_scores.append(np.mean(bin_rougeL))\n",
    "        bin_counts.append(len(indices))\n",
    "    else:\n",
    "        bin_rouge1_scores.append(0)\n",
    "        bin_rouge2_scores.append(0)\n",
    "        bin_rougeL_scores.append(0)\n",
    "        bin_counts.append(0)\n",
    "\n",
    "# Visualize performance by article length\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "x_pos = np.arange(len(bin_labels))\n",
    "plt.bar(x_pos, bin_rouge1_scores, alpha=0.7, label='ROUGE-1')\n",
    "plt.bar(x_pos, bin_rouge2_scores, alpha=0.7, label='ROUGE-2')\n",
    "plt.bar(x_pos, bin_rougeL_scores, alpha=0.7, label='ROUGE-L')\n",
    "plt.title('ROUGE Scores by Article Length')\n",
    "plt.xlabel('Article Length Category')\n",
    "plt.ylabel('ROUGE Score')\n",
    "plt.xticks(x_pos, bin_labels, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(x_pos, bin_counts, alpha=0.7, color='orange')\n",
    "plt.title('Sample Count by Article Length')\n",
    "plt.xlabel('Article Length Category')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(x_pos, bin_labels, rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(article_lengths, rouge1_scores, alpha=0.6)\n",
    "plt.title('ROUGE-1 vs Article Length')\n",
    "plt.xlabel('Article Length (words)')\n",
    "plt.ylabel('ROUGE-1 Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Performance Analysis by Article Length:\")\n",
    "for i, (label, count, r1, r2, rL) in enumerate(zip(bin_labels, bin_counts, bin_rouge1_scores, bin_rouge2_scores, bin_rougeL_scores)):\n",
    "    if count > 0:\n",
    "        print(f\"{label}: {count} samples - ROUGE-1: {r1:.4f}, ROUGE-2: {r2:.4f}, ROUGE-L: {rL:.4f}\")\n",
    "\n",
    "# Identify best and worst performing examples\n",
    "best_idx = np.argmax(rouge1_scores)\n",
    "worst_idx = np.argmin(rouge1_scores)\n",
    "\n",
    "print(f\"\\nBest Performing Example (ROUGE-1: {rouge1_scores[best_idx]:.4f}):\")\n",
    "print(f\"Article: {test_articles[best_idx][:200]}...\")\n",
    "print(f\"Reference: {test_summaries[best_idx]}\")\n",
    "print(f\"Generated: {decoded_preds[best_idx]}\")\n",
    "\n",
    "print(f\"\\nWorst Performing Example (ROUGE-1: {rouge1_scores[worst_idx]:.4f}):\")\n",
    "print(f\"Article: {test_articles[worst_idx][:200]}...\")\n",
    "print(f\"Reference: {test_summaries[worst_idx]}\")\n",
    "print(f\"Generated: {decoded_preds[worst_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Conclusions\n",
    "\n",
    "### Model Performance Summary\n",
    "- **Base Model**: BART-Large-CNN (facebook/bart-large-cnn)\n",
    "- **Fine-tuning Method**: LoRA (Low-Rank Adaptation) for parameter efficiency\n",
    "- **Dataset**: CNN/DailyMail summarization dataset\n",
    "- **Evaluation Metrics**: ROUGE-1, ROUGE-2, ROUGE-L, BLEU\n",
    "\n",
    "### Key Results\n",
    "- **ROUGE-1**: [Filled after evaluation] - Measures unigram overlap\n",
    "- **ROUGE-2**: [Filled after evaluation] - Measures bigram overlap  \n",
    "- **ROUGE-L**: [Filled after evaluation] - Measures longest common subsequence\n",
    "- **BLEU**: [Filled after evaluation] - Measures n-gram precision\n",
    "\n",
    "### Technical Highlights\n",
    "1. **Parameter-Efficient Training**: Used LoRA to fine-tune only a small subset of parameters\n",
    "2. **Memory Optimization**: Applied 4-bit quantization for resource-constrained environments\n",
    "3. **Generation Quality**: Used beam search with length penalty for better summary quality\n",
    "4. **Comprehensive Evaluation**: Multi-metric evaluation with detailed analysis\n",
    "\n",
    "### Strengths\n",
    "1. **Domain Adaptation**: Successfully adapted pre-trained model to specific summarization task\n",
    "2. **Efficiency**: LoRA enables fine-tuning with minimal computational resources\n",
    "3. **Flexibility**: Model can generate summaries of varying lengths and styles\n",
    "4. **Evaluation**: Comprehensive metrics provide detailed performance insights\n",
    "\n",
    "### Limitations and Areas for Improvement\n",
    "1. **Dataset Size**: Limited training data may affect generalization\n",
    "2. **Factual Accuracy**: Model may generate plausible but incorrect information\n",
    "3. **Length Control**: Difficulty in precisely controlling summary length\n",
    "4. **Domain Specificity**: Performance may vary on different text domains\n",
    "\n",
    "### Future Improvements\n",
    "1. **Larger Models**: Use actual LLaMA 3.1 or other state-of-the-art models when available\n",
    "2. **Multi-Document Summarization**: Extend to summarize multiple related articles\n",
    "3. **Controllable Generation**: Add control tokens for length, style, and focus\n",
    "4. **Fact Verification**: Integrate fact-checking mechanisms to improve accuracy\n",
    "5. **Domain Adaptation**: Fine-tune on domain-specific datasets (medical, legal, etc.)\n",
    "\n",
    "### Clinical and Research Applications\n",
    "1. **Medical Literature Review**: Summarize research papers and clinical studies\n",
    "2. **Patient Report Summarization**: Generate concise summaries of lengthy medical records\n",
    "3. **News Monitoring**: Track and summarize healthcare-related news and developments\n",
    "4. **Educational Content**: Create summaries for medical education and training\n",
    "\n",
    "This summarization model demonstrates effective fine-tuning of transformer models for abstractive text summarization, providing a foundation for automated content processing in healthcare and other domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}